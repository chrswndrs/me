# Notes for 2023-07-19

## 13:41

Introduction
You are provided with a partially completed Terraform configuration and existing resources in AWS managed by Terraform for this Lab. Your goal is to achieve the following:

Configure existing instances to serve a custom website
Create an Elastic Load Balancer with the instances in its pool
Create public subnets and security groups to secure the deployment
This Lab Step focuses on the first item. You will modify the existing configuration to configure the instances to serve a website. For demonstration purposes, the website is a simple web page that echoes back the instance ID of the EC2 instance that served the request.

 

Instructions
1. Change into the Terraform working directory called infra:

Copy code
cd infra
 

2. List the resources currently managed by Terraform:

Copy code
terraform state list
alt

There is a VPC, two subnets, and two instances. The subnets are private, i.e. they don't have a route to the internet. The instances are running the Apache web server and a default web page is being served.

 

3. Remove the current instance configuration in the main.tf file:

Copy code
sed -i '/.*aws_instance.*/,$d' main.tf
This command deletes all the lines from the file starting from the line matching aws_instance. The instance configuration is the last block in the file so all the other configuration is preserved.

 

4. Append the following resource block to configure the instances to serve a custom website that echoes back the instance's ID:

Copy code
cat >> main.tf <<'EOF'
resource "aws_instance" "web" {
  count         = "${var.instance_count}"
  # lookup returns a map value for a given key
  ami           = "${lookup(var.ami_ids, "us-west-2")}"
  instance_type = "t2.micro"
  # Use the subnet ids as an array and evenly distribute instances
  subnet_id     = "${element(aws_subnet.web_subnet.*.id, count.index % length(aws_subnet.web_subnet.*.id))}"
  
  # Use instance user_data to serve the custom website
  user_data     = "${file("user_data.sh")}"
  
  tags {
    Name = "Web Server ${count.index + 1}"
  }
}

EOF
The configuration uses user_data to bootstrap the instances to serve a custom website. The file built-in interpolation function reads the contents of the file into a string. This keeps the resource block clean by keeping the bootstrapping commands in a separate file that you will create next. Terraform also supports provisioners which also run commands when a remote machine is created. Provisioners are ideal for initializing configuration management tools like Chef. User data is sufficient for the relatively simple bootstrapping required for this Lab. It also has the benefit of being retrievable from within AWS.

 

5. Create the user_data.sh script that will create and serve the custom website:

Copy code
cat >> user_data.sh <<'EOF'
#!/bin/bash
cat > /var/www/html/index.php <<'END'
<?php
$instance_id = file_get_contents("http://instance-data/latest/meta-data/instance-id");
echo "You've reached instance ", $instance_id, "\n";
?>
END
EOF
The script creates a file called index.php in the default serving directory of the Apache web server. The PHP code gets the instance ID from the instance's metadata and echoes it back to the user.

 

6. View the execution plan for the configuration change, and enter yes when prompted:

Copy code
terraform apply
alt

The plan tells you that Terraform must destroy and then create replacement instances. This is because the user_data must be executed when an instance is first launched. Many changes to instance configuration don't require recreation. Terraform would notify you of an update in-place in the execution plan when recreation isn't required. The operation should complete in under one minute.

 

Summary
In this Lab Step, you used AWS EC2 instance user data to bootstrap instances to serve a custom website. Modifying the user_data argument in Terraform requires recreating instances. You saw how Terraform's execution clarifies when the recreation of a resource is required.

## 13:42

Introduction
You now have two instances serving your custom website. The instances are running in private subnets in different availability zones and are assigned to the default security group for the VPC that allows all traffic. There are a few resources that need to be created to allow for an Elastic Load Balancer (ELB) to securely distribute traffic between the instances:

Public subnets for each availability zone must be created so the load balancer can be accessed from the internet
This requires additional resources such as an internet gateway to connect to the internet, and route tables that route to the internet
A security group to allow traffic from the internet to the public subnets that will house the ELB on port 80 (HTTP)
A security group to allow traffic from the ELB in the public subnets to the instances in the private subnets on port 80 (HTTP)
You will make use of separate configuration files to make the configuration more manageable.

 

Instructions
1. Create the required networking resources for public subnets in a configuration file named networking.tf:

Copy code
cat > networking.tf <<'EOF'
# Internet gateway to reach the internet
resource "aws_internet_gateway" "web_igw" {
  vpc_id = "${aws_vpc.web_vpc.id}"
}
# Route table with a route to the internet
resource "aws_route_table" "public_rt" {
  vpc_id = "${aws_vpc.web_vpc.id}"
  
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = "${aws_internet_gateway.web_igw.id}"
  }
  tags {
    Name = "Public Subnet Route Table"
  }
}
# Subnets with routes to the internet
resource "aws_subnet" "public_subnet" {
  # Use the count meta-parameter to create multiple copies
  count             = 2
  vpc_id            = "${aws_vpc.web_vpc.id}"
  cidr_block        = "${cidrsubnet(var.network_cidr, 2, count.index + 2)}"
  availability_zone = "${element(var.availability_zones, count.index)}"
  tags {
    Name = "Public Subnet ${count.index + 1}"
  }
}
# Associate public route table with the public subnets
resource "aws_route_table_association" "public_subnet_rta" {
  count          = 2
  subnet_id      = "${aws_subnet.public_subnet.*.id[count.index]}"
  route_table_id = "${aws_route_table.public_rt.id}"
}
EOF
Read through the configuration to get an understanding of how to configure each resource. The VPC for the website deployment is named web_vpc, and the availability zones of the two instances are stored in a list variable named availability_zones in the variables.tf file.

 

2. Execute the plan to create the networking resources:

Copy code
terraform apply
alt

The command should complete in under a minute.

 

3. Create the security groups that will secure traffic into the public and private subnets in a configuration file called security.tf:

Copy code
cat > security.tf <<'EOF'
resource "aws_security_group" "elb_sg" {
  name        = "ELB Security Group"
  description = "Allow incoming HTTP traffic from the internet"
  vpc_id      = "${aws_vpc.web_vpc.id}"
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  # Allow all outbound traffic
  egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
resource "aws_security_group" "web_sg" {
  name        = "Web Server Security Group"
  description = "Allow HTTP traffic from ELB security group"
  vpc_id      = "${aws_vpc.web_vpc.id}"
  # HTTP access from the VPC
  ingress {
    from_port       = 80
    to_port         = 80
    protocol        = "tcp"
    security_groups = ["${aws_security_group.elb_sg.id}"]
  }
  # Allow all outbound traffic
  egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
EOF
Read through the configuration to understand how the resources are configured. For simplicity, the egress rules for both security groups allow outbound traffic to anywhere.

 

4. Apply the configuration changes to create the security groups:

Copy code
terraform apply
alt

The plan only takes a few seconds to apply.

 

5. Remove the current instance configuration in the main.tf file so you can modify the configuration to attach the web server security group to them:

Copy code
sed -i '/.*aws_instance.*/,$d' main.tf
 

6. Append the following resource block to configure the instances to serve a custom website that echoes back the instance's ID:

Copy code
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
cat >> main.tf <<'EOF'
resource "aws_instance" "web" {
  count                  = "${var.instance_count}"
  # lookup returns a map value for a given key
  ami                    = "${lookup(var.ami_ids, "us-west-2")}"
  instance_type          = "t2.micro"
  # Use the subnet ids as an array and evenly distribute instances
  subnet_id              = "${element(aws_subnet.web_subnet.*.id, count.index % length(aws_subnet.web_subnet.*.id))}"
  
  # Use instance user_data to serve the custom website
  user_data              = "${file("user_data.sh")}"
  
  # Attach the web server security group
  vpc_security_group_ids = ["${aws_security_group.web_sg.id}"]
  tags { 
    Name = "Web Server ${count.index + 1}" 
  }
}
EOF
The only difference from the previous configuration is the addition of vpc_security_group_ids.

 

7. Apply the configuration change:

Copy code
terraform apply
alt

This change doesn't require recreation and an update in-place can be performed.

 

Summary
In this Lab Step, you created two additional configuration files to manage network and security resources for the public and private subnets. The public subnets will house the ELB that you create in the next Lab Step. The public subnet security group allows incoming HTTP traffic from the internet, and the private subnet security group allows incoming HTTP traffic from the ELB public subnet security group.

## 13:48

Introduction
You now have completed the private tier of the website infrastructure and you have two public subnets and a security group for an ELB. In this Lab Step, you will complete the scenario by adding a cross-zone ELB to distribute traffic to the web servers running in the private subnets. The website will then be highly available since the ELB will allow you to continue serving the website while one instance is down.

 

Instructions
1. Create an ELB configuration in a file named load_balancer.tf:

Copy code
cat > load_balancer.tf <<'EOF'
resource "aws_elb" "web" {
  name = "web-elb"
  subnets = ["${aws_subnet.public_subnet.*.id}"]
  security_groups = ["${aws_security_group.elb_sg.id}"]
  instances = ["${aws_instance.web.*.id}"]

  # Listen for HTTP requests and distribute them to the instances
  listener { 
    instance_port     = 80
    instance_protocol = "http"
    lb_port           = 80
    lb_protocol       = "http"
  }

  # Check instance health every 10 seconds
  health_check {
    healthy_threshold = 2
    unhealthy_threshold = 2
    timeout = 3
    target = "HTTP:80/"
    interval = 10
  }
}

EOF
The ELB is internet-facing by default. You can create an internal ELB by adding an internal argument and setting it to true. The ELB configuration documentation is available at this link.

 

2. Add an output that will save the DNS address of the ELB as the website address:

Copy code
cat >> outputs.tf <<'EOF'
output "site_address" {
  value = "${aws_elb.web.dns_name}"
}

EOF
 

3. Apply the configuration changes:

Copy code
terraform apply
alt

 

4. Store the site_address output in a shell variable:

Copy code
site_address=$(terraform output site_address)
 

5. Send an HTTP request to the ELB every two seconds using the watch and curl command:

Copy code
watch curl -s $site_address
alt

It takes about a minute for the ELB to start sending requests to the instances. You will eventually see messages from the web server instances and you should notice the instance ID changing between two values. This verifies the ELB is distributing the traffic to all of the instances.

 

6. Press ctrl+c to stop the watch command..

 

Summary
In this Lab Step, you configured a cross-zone Elastic Load Balancer in the public subnets of your infrastructure. The load balancer distributes traffic to the web server instances in the private subnets of your infrastructure. You used an output to conveniently access the DNS name of the load balancer.

## 13:51

Introduction
Terraform provides the destroy command to delete resources managed by Terraform that you no longer want.  You can destroy specific resources or all of the resources at once. You will destroy everything managed by Terraform in this Lab Step.

 

Instructions
1. Delete the ELB security group using the destroy command and the target option, and enter yes to accept the plan:

Copy code
terraform destroy -target=aws_security_group.elb_sg
alt

To destroy the target resource, all the resources that depend on the target must also be destroyed. You can see the ELB security group highlighted in the following dependency graph:

alt

All of the resources above the security group are the ones that are destroyed. The web_sg security group depends on the elb_sg because it has a rule that only allows traffic from the elb_sg security group.

 

2. Run the plan command to verify that destroying resources doesn't affect your desired configuration:

Copy code
terraform plan
alt

The plan is to add back the resources you just destroyed. This may be what you want if a resource is modified outside of Terraform and it can't be brought back to the desired state requiring you to destroy and recreate it. If you actually no longer need a resource, you should remove it from the configuration.

 

3. Destroy all the remaining resources managed by Terraform, and enter yes when prompted:

Copy code
terraform destroy
alt

 

Summary
In this Lab Step, you learned how to destroy resources managed by Terraform. You used both flavors of destroy: destroying specific targets, and destroying all resources managed by Terraform.

## 14:38

Introduction
This Lab focuses on demonstrating the capabilities of static analysis of Infrastructure as Code (IaC) and automated alerting based on the static analysis results. For the sake of the Lab, a specific IaC tool must be selected. Terraform is the IaC tool used in this Lab. The Lab uses static analysis tools specifically for Terraform, although comparable tools often exist for alternative IaC tools, such as AWS CloudFormation, Chef, or Ansible.

This Lab Step sets up Terraform configuration files in your AWS Cloud9 IDE. The configuration is for a highly-available website deployed in AWS. Although Terraform can orchestrate infrastructure on multiple clouds, the static analysis tools for Terraform are most mature for AWS. This Lab does not focus on explaining the configuration, and it is not required to have prior experience with Terraform to complete this Lab. Other Terraform AWS Labs on Cloud Academy go through the process of building and understanding the configuration. Look at the following environment diagram to understand the major AWS resources maintained by the configuration: 

alt

At a high level, the infrastructure load balances website traffic across multiple availability zones. Web servers are in private subnets as a security best practice. Only traffic from the load balancer in the public subnets can reach the web servers.

 

Instructions
1. In the AWS Cloud9 terminal at the bottom of the window, enter the following command to download the configuration to a zip archive named tf.zip:

Copy code
wget https://github.com/cloudacademy/terraform-highly-available-website-on-aws/raw/master/config.zip -O tf.zip
 

2. Extract the archive to a directory named tf, remove the zip file, and change into the tf directory:

Copy code
unzip -d tf tf.zip
rm tf.zip
cd tf
alt

The archive includes four files: three Terraform configuration files ending in .tf, and one shell script used to bootstrap a basic website on web server instances.

 

3. In the Environment tab on the left of the Cloud9 IDE, expand the tf directory and open the main.tf file:

alt

main.tf is often used as the name of the root configuration file in Terraform, by convention.

 

4. Briefly scan the file to see the types of AWS resources, which are listed after lines beginning with the keyword resource:

alt

That is all you need to understand in the Terraform configuration files for a basic understanding of the IaC. Terraform interprets these files and makes appropriate API calls to create and update the infrastructure based on the configuration files.

Tip: Cloud9 includes syntax highlighting for HashiCorp Configuration Language (HCL) that Terraform configuration files are written in. It should automatically be enabled. You can confirm this by viewing the View > Syntax menu and verifying that Terraform is selected.

 

Summary
In this Lab Step, you downloaded and briefly reviewed a sample Terraform IaC configuration.

## 14:40

Introduction
Terraform ships with built-in commands that perform static analysis functions. Core to Terraform's workflow is the separation of creating an execution plan and actually applying the plan to the environment. The execution plan describes what changes need to happen in the environment to match the desired state described in the configuration files. The terraform plan command generates an execution plan. If any errors are detected, the output of the command will state them. The plan command is essentially performing a kind of static analysis. The terraform apply command takes the execution plan and actually applies the changes. 

There are also a couple other Terraform commands that perform static analysis and you will use them in this Lab Step.

 

Instructions
1. Install the Terraform CLI by issuing the following commands:

Copy code
wget https://releases.hashicorp.com/terraform/0.11.3/terraform_0.11.3_linux_amd64.zip -O /tmp/tf.zip
sudo unzip /tmp/tf.zip -d /usr/local/bin
 

2. Check the formatting of the configuration files to see if they follow the canonical Terraform style:

Copy code
terraform fmt -check
alt

The output lists files that are not in the canonical Terraform style are listed. The format (fmt) command recursively looks for Terraform configuration files. Adding the -diff option outputs the changes that need to be made to make the files canonical.

 

3. Print the exit code of the last command

Copy code
echo $?
alt

The fmt command with the check option, returns a non-zero exit code if it finds any files that are not formatted correctly. This can be used in automation; for example, in a pre-commit hook in your source control system or in a continuous integration pipeline to ensure proper formatting.

 

4. Canonically style the configuration files:

Copy code
terraform fmt
The fmt command without any options automatically modifies any configuration files to match the canonical style. This can also be useful as a pre-commit hook, if desired.

 

5. Click on the main.tf editor tab, and click Reload if you are prompted about changes to the file:

alt

 

6. Validate the configuration and working directory:

Copy code
terraform validate
alt

The validate command performs syntax checks, such as checking for valid structure in the file, properly spelled keywords, and valid resource references. fmt also checks for valid structure, such as matching opening and closing braces, but will not detect anything more. As the output shows, validate also performs some checks dependencies and ensures configuration variables are defined. There is a missing dependency in this case; the AWS provider plugin is not installed.

 

7. Attempt to generate a Terraform execution plan by entering the following command:

Copy code
terraform plan
alt

The plan command detects the same issue and reports the same output as validate in red. This is because the plan command includes the same code as the validate command.

 

8. Initialize the working directory to correct the error:

Copy code
terraform init
alt

Terraform initializes the directory by downloading the AWS provider plugin into a .terraform subdirectory. The provider plugin wraps the AWS API calls that Terraform needs to manage AWS resources.

 

9. Re-attempt to generate a Terraform execution plan:

Copy code
terraform plan
alt

Terraform has detected an invalid resource that is a result of misspelling gateway as getaway.

 

10. Edit the main.tf file to replace the two occurrences of aws_internet_getaway with aws_internet_gateway.

There is one on line 33 and one on line 43:

alt

 

11. Save the main.tf file by clicking on File > Save in the upper menu:

alt

 

12. Return to the terminal tab, and generate a Terraform execution plan:

Copy code
terraform plan
alt

The command writes a lot of output describing the plan to create the environment described by the configuration files. The lines in the image summarize the plan to add 14 resources. Note that no errors or warnings are reported. A non-zero exit code is returned if any errors are detected. If you were really creating the infrastructure, you would proceed to use Terraform's apply command to create the environment. Depending on the configuration and resources involved, the apply command can take several minutes.

 

Summary
In this Lab Step, you learned about the built-in static analysis capabilities in Terraform. You should also have an understanding of the scope and extent of the built-in checks performed by the built-in commands.

## 14:51

Introduction
Terrascan is another open-source, static analysis tool for Terraform configurations. Terrascan is focused more on security best practices and only supports AWS resources. Cloud environments often have security auditing tools to check the security of active environments. Some examples are AWS Config and Microsoft Azure's Security Center. There are also third-party auditing tools that can perform a variety of security checks on your active environments such as Scout2 for AWS and G-Scout for Google Cloud. Terrascan is different in that it aims to secure your environment even before it is created. Both kinds of tools are useful and both are an essential part of a defense-in-depth security strategy.

You will install and use Terrascan to check the security of the sample Terraform IaC environment in this Lab Step.

 

Instructions
1. Enter the following commands to install Terrascan in a Python virtual environment:

Copy code
cd
virtualenv scan
source scan/bin/activate
pip install -Iv terrascan==0.1.0
cd environment/tf
alt

You can ignore the red error message since it does not impact your ability to use Terrascan. You will see (scan) at the beginning of your shell prompt to indicate you are using the scan virtual environment. The virtual environment avoids any dependency conflicts with pre-installed Python packages, and uses Python version 3 instead of the system default of Python version 2.

 

2. List the options for Terrascan:

Copy code
terrascan -h
alt

The options are simply the location of the configuration files to scan and the tests to perform. The supported test groups are encryption, logging_and_monitoring, public_exposure, and security_group. You can also specify all to run all of the test groups. You can appreciate the security focus of Terrascan based on the test groups.

 

3. Run the Terrascan security group tests:

Copy code
terrascan --location . --tests security_group
alt

The test group Ran 5 tests and one FAILED. The test_aws_security_group_inline_rule_open failed because the ingress cidr_blocks array for the load balancer includes all IPv4 addresses (0.0.0.0/0). This means incoming traffic can originate from anywhere. There are risks to that, but, for the website being deployed, that is an acceptable risk. Neither Terraform nor TFLint presented this warning. You will not correct any potential issues raised by Terrascan in this Lab. Terrascan will return a non-zero exit code if any tests fail, making it suitable for automation.

 

4. Deactivate the virtual environment:

Copy code
deactivate
 

Summary
In this Lab Step, you used an open-source, security-focused, static analysis tool for Terraform named Terrascan. You learned about the types of checks that Terrascan can perform, and how it can be used in automation.

## 14:51

Introduction
Static code analysis tools are helpful during the development stage. But because it can be difficult to ensure all developers are using the same tools and configurations all the time, it is useful to include the static analysis tools in your continuous integration pipeline. If any errors are detected, the build system can notify relevant teams so the errors can be addressed.

In this Lab, you will use Jenkins for automating the continuous integration pipeline and alerting. You will configure a Jenkins server that has been started and initialized by the Cloud Academy Lab environment. Jenkins is configured to allow anonymous access to simplify the Lab process. You will make a project in Jenkins that watches for changes to a Git repository and runs static analysis whenever new code is committed. In this Lab, the code is only Terraform configuration files. It is possible to have application code and IaC in the same repository allowing you to automate more. In that case, you could build the application and deploy it to a test environment created using the IaC.

 

Instructions
1. Enter the following command in the Cloud9 terminal to get the URL for Jenkins:

Copy code
aws ec2 describe-instances \
  --filters "Name=tag:Type,Values=Build" \
  --query "Reservations[0].Instances[0].PublicDnsName" \
  | sed 's/"\(.*\)"/http:\/\/\1\/manage/'
 alt

The command finds the instance with a Type tag equal to Build to find the Jenkins server.

 

2. Copy the Jenkins URL.

 

3. In a new browser tab, navigate to the Jenkins URL you copied earlier.

 

4. Click New Item to begin configuring the project:

alt

 

5. In the Enter an item name field enter terraform-lab, then click Freestyle project and OK:

alt

 

6. In the configuration wizard, enter the following values and accept the defaults for the others:

Source Code Management: Git
Repositories
Repository URL: git://localhost/lab.git
Build Triggers: Poll SCM
Build: Click Add build step > Execute shell
Execute shell
Command: 
Copy code
#!/bin/bash
docker run -v $(pwd):/src --workdir=/src --rm wata727/tflint:0.5.4 --error-with-issues
alt

The Cloud Academy Lab environment has configured the lab.git repository and a post-receive Git hook to trigger the build in Jenkins when new code is pushed to it. The build step uses Docker to run tflint. Only TFLint is used for demonstration purposes. You could also configure other build steps for Terraform's static analysis commands. You could even have Terraform apply the changes if all the steps passed. The options passed to Docker have the following meanings:

-v $(pwd):/src: Mount the repository's source code into the /src directory inside the container
--workdir=/src: Set the working directory inside the container to /src, where the configuration files are mounted
--rm: Remove the container after it exits
wata727/tflint:0.5.4: The Docker image for TFLint version 0.5.4
--error-with-issues: The option for TFLint to return a non-zero exit code, if any issue is detected.
The --ignore-rule option is omitted to intentionally cause the build to fail. 

 

7. Click Save in the lower-left corner to save the project:

alt

Jenkins is now ready to automatically scan the Terraform configuration files when you push changes to the Git repository it is watching.

 

Summary
In this Lab Step, you configured a Jenkins freestyle project that automates running TFLint whenever new code is checked into a Git repository.

## 14:54

Introduction
Jenkins is ready to respond to any code push you send to a Git repository set up on the build server. In this Lab Step, you will trigger a Jenkins build by pushing the Terraform configuration you have worked with in prior Lab Steps.

 

Instructions
1. In the Cloud9 terminal tab, enter the following commands to clone the Git repository on the build server to a directory named src:

Copy code
cd ~/environment
repo_url=$(aws ec2 describe-instances --filters "Name=tag:Type,Values=Build" --query "Reservations[0].Instances[0].PublicDnsName" \
           | sed 's/"\(.*\)"/git:\/\/\1\/lab.git/')
git clone $repo_url src
alt

The repository is initially empty so the warning is expected and not an issue.

 

2. Copy the Terraform configuration files, and commit them to your local clone of the Git repository:

Copy code
cp tf/*tf tf/*sh src
cd src
git add -A
git commit -m "Initial commit"
alt

The output explains that it is using an auto-generated identity for your commit. That is all right for the Lab.

 

3. Push the commit to the remote Git repository on the build server:

Copy code
git push
alt

The remote: messages are output generated when running the post-receive Git hook on the build server. The first message tells you the polling build trigger is being used.

 

4. Refresh the Jenkins browser tab:

alt

Notice that there is a #1 build in the Build History on the left. The red ball indicates the build failed. There are also Permalinks in the center that link to the most recent builds of different categories.

 

5. Hover over #1 to expose a menu arrow to click and select Console Output:

alt

 

6. Quickly read through the output and notice the Terrascan output in the lower half:

alt

The NOTICE is causing the exit code to be non-zero. Jenkins detects the non-zero exit code and reports the build step as failure and the entire build Finished: FAILURE. That is the desired behavior. Any Terraform configuration that does not pass the static analysis will not be able to reach further build stages, such as being deployed with a terraform apply.

 

7. Click Back to Project to return to the main project view:

alt

 

Summary
In this Lab Step, you triggered an automatic Jenkins build by pushing code to the build server's Git repository. The build failed as a result of the TFLint not running cleanly.

## 14:57

Introduction
You now have static analysis of your IaC integrated into your continuous integration pipeline. The problem now is that you would never know the build failed unless you go to the Jenkins page. You will add alerting to automatically notify you of the build result.

Jenkins has a variety of plugins to support a range of notification requirements. For example, Jenkins can directly send emails, or post a message to a Slack channel. You will configure Jenkins to publish a message to an Amazon Simple Notification Service (SNS) topic. You will create the SNS topic and subscribe to it using your email address in this Lab Step.

 

Instructions
1. In the AWS Management Console search bar, enter SNS, and click the Simple Notification Service result under Services:

alt

 

2. If you see the following welcome page, click Topics on the left-side menu:

alt

 

3. Click Create topic:

alt

 

4. In the Create new topic form, enter the following values and click Create topic:

Type: Standard
Name: build-results
Display name: build
alt

 

5. Copy the Amazon Resource Name (ARN) in the Topic details:

alt

You need the ARN in order to identify the topic you want to subscribe to.

 

6. From the Topic dashboard, click Create subscription:

alt

 

7. In the Create subscription form, enter the following values and click Create subscription:

Protocol: Email
Endpoint: Enter your valid email address (for example, john.doe@cloudacademy.com).
alt

 

8. Notice that the subscription is pending confirmation in the Topic details page:

alt

 

9. Open your email client. Find and open the new email from the SNS topic you created; AWS Notification is part of the the subject line:

alt

 

10. Click the Confirm subscription link:

alt

 You will see a Subscription confirmed screen similar to the following:

alt

 

11. Return to the Topic details page for the SNS topic you created earlier.

Notice the Subscription ARN changed from PendingConfirmation to the actual Subscription ID.

 

Summary
In this Lab Step, you created and subscribed to an SNS topic that will be used to receive build notifications.
