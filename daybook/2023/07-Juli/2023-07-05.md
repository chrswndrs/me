# Notes for 2023-07-05

## 12:24

Introduction

Provisioners allow additional steps to be taken when provisioning resources in Terraform. They can provide a great solution for additional automation steps,  however, it is important to understand that provisioners introduce more complexity into the Terraform code and should be used as a last resort.

In this lab step, you will explore using a provisioner block when developing Terraform code. 

2. Right-click on the terraformlab folder and click New File:

3. Name the file main.tf and click OK:

It's common practice to name Terraform configuration files "main". You can also create multiple .tf files in a directory which will be explored in future labs.

4. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
resource "aws_ecr_repository" "ecr" {
  name = "catest"
}
#Import Container Image to Elastic Container Registry
resource "null_resource" "image" {
  provisioner "local-exec" {
    command = <<EOF
       $(aws ecr get-login --region us-west-2 --no-include-email)
       docker pull hello-world:latest
       docker tag hello-world:latest ${aws_ecr_repository.ecr.repository_url}
       docker push ${aws_ecr_repository.ecr.repository_url}:latest
   EOF
  }
}
This configuration will create an Elastic Container Registry in your AWS account. Notice there is a null_resource resource block with a provisioner block inside. This is an empty resource that will run a series of commands on the local endpoint running Terraform. When the code is executed in this main.tf file, it will create an ECR resource and then perform the commands stated in the provisioner block to automatically upload a Docker image to the container registry. 

Note: The Docker client and AWS CLI tools have been installed in this IDE session, or the provisioner block would not work since the commands are executed locally. This is one of the reasons why Provisioner blocks are not a good idea. They implement dependencies on the code. Now, this main.tf cannot be run on just any endpoint. It needs to have additional tools installed. 

The reference to the ECR repository URL creates an implicit dependency inside the provisioner block that is telling Terraform to create the null_resource block and run the provisioner block after the ECR resource has been created:

5. Save the main.tf file by clicking File in the upper left corner and click save:

6. Left-click on the Terminal menu and click New Terminal:

A Terminal window will appear at the bottom.

7. Inside the terminal, input the following command to change directory to the terraformlab folder:

cd terraformlab
 

8. Initialize the directory for Terraform by inputting the terraform init command into the terminal:

terraform init

The AWS provider is downloaded during the initialization process. 

9. Run terraform apply and input yes to deploy the config:

terraform apply

You can see that the ECR resource is created and then the provisioner block commands are executed pushing the hello-world image up to the repository. The provisioner allows us to automate more than just the ECR resource in Terraform and create infrastructure code that produces exactly what is needed in one command.

 

10. Add a provisioner block to the aws_ecr_repository resource in the main.tf file:

resource "aws_ecr_repository" "ecr" {
  name = "catest"
  provisioner "local-exec" {
    when = destroy
    command = <<EOF
    $(aws ecr get-login --region us-west-2 --no-include-email)
    docker pull ${self.repository_url}:latest
    docker save --output catest.tar ${self.repository_url}:latest 
    EOF
  }
}
You can add provisioner blocks inside of resources. However, it's best practice to create a null resource block when possible. This provisioner block also has the when = destroy attribute, which tells Terraform only to run this only when the resource is being destroyed. Also, the self.repository_url is being interpolated within the provisioner block to reference the URL of the ECR resource.

Running terraform destroy will execute the commands inside the provisioner block, pull the latest image from the repository, and save it as a tar file before deleting the ECR resource. 

The entire main.tf file should look like the following:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
resource "aws_ecr_repository" "ecr" {
  name = "catest"
  provisioner "local-exec" {
    when = destroy
    command = <<EOF
    $(aws ecr get-login --region us-west-2 --no-include-email)
    docker pull ${self.repository_url}:latest
    docker save --output catest.tar ${self.repository_url}:latest 
    EOF
  }
}
#Import Container Image to Elastic Container Registry
resource "null_resource" "image" {
  provisioner "local-exec" {
    command = <<EOF
       $(aws ecr get-login --region us-west-2 --no-include-email)
       docker pull hello-world:latest
       docker tag hello-world:latest ${aws_ecr_repository.ecr.repository_url}
       docker push ${aws_ecr_repository.ecr.repository_url}:latest
   EOF
  }
}
 

11. Save the main.tf file.

 

12. Run terraform destroy in the terminal and input yes:

Copy code
1
terraform destroy
The docker commands inside the provsioner block  are ran before the ECR resource is destroyed. When checking the directory, the catest.tar is now present in the directory:

alt

Keep in mind that provisioners are extremely useful for adding additional tasks to deploying and destroying resources. However, provisioners should be a last resource because it adds complexity to Terraform code and can make it brittle. It's best to seek alternative solutions before using provisioners. In this example, instead of pushing the docker image through a Terraform provisioner, it would be best to use a CI/CD tool like Azure DevOps or Jenkins to perform that task and let the Terraform code handle the infrastructure. 

 

Summary
In this lab step, you created a provisioner to upload a Docker image to an ECR resource after it was provisioned. You also created a destroy provisioner to perform clean up tasks when the ECR was destroyed.

## 12:40

Introduction

Terraform state is the database it keeps for mapping AWS resources to the Terraform configuration file. It is a critical component of the way Terraform works. Inadequate understanding of the way Terraform uses state can result in poor automated solutions or even production outages.

In this lab step, you will explore Terraform state and the importance of managing state.

3. Name the file main.tf and click OK:

In the lower right corner, you will see some pop-ups from the Terraform Visual Studio Code extension. These can be ignored:

4. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
resource "aws_vpc" "example" {
  cidr_block = "10.0.0.0/16"
}
resource "aws_subnet" "example" {
  vpc_id     = aws_vpc.example.id
  cidr_block = "10.0.1.0/24"
  availability_zone = "us-west-2a"
  tags = {
    Name = "calabvm-subnet"
  }
}
resource "aws_instance" "server1" {
    ami           = "ami-01fee56b22f308154"
    instance_type = "t2.micro"
    monitoring = true
    vpc_security_group_ids = [aws_vpc.example.default_security_group_id]
    subnet_id = aws_subnet.example.id               
    root_block_device {
        delete_on_termination = false
        encrypted = true
        volume_size = "8"
        volume_type = "standard"
    }
    tags = {
        Name = "calabvm1"
    }
}
resource "aws_instance" "server2" {
    ami           = "ami-01fee56b22f308154"
    instance_type = "t2.micro"
    monitoring = true
    vpc_security_group_ids = [aws_vpc.example.default_security_group_id]
    subnet_id = aws_subnet.example.id               
    root_block_device {
        delete_on_termination = false
        encrypted = true
        volume_size = "8"
        volume_type = "standard"
    }
    tags = {
        Name = "calabvm2"
    }
}
The configuration will deploy a VPC, Subnet, and two EC2 instances.

 

5. Save the main.tf file by clicking File in the upper left corner and click save:


6. Left-click on the Terminal menu and click New Terminal:

A Terminal window will appear at the bottom.

7. Inside the terminal, input the following command to change directory to the terraformlab folder:

cd terraformlab
 

8. Initialize the directory for Terraform by inputting the terraform init command into the terminal:

terraform init

The AWS provider is downloaded during the initialization process. 

 

9. Run terraform apply:

terraform apply

The terraform.tfstate file is created but not populated yet with data:

10. Input yes to deploy the config.

The AWS resources are created:

11. On the left-hand side, click on the terraform.tfstate file:

The file will be opened in a new tab in the IDE. The state file contains the resources that were deployed into AWS. Taking a closer look you can see that the state file is mapping the Server1 resource block label that was specified in the Terraform configuration file with the AWS ARN identifier. The state file also pulls down the additional attributes of the resource as well like cpu_core_count:

Note: The state file can also contain sensitive information, which is why it is important to never store the terraform.tfstate file in a public git repository or S3 bucket. Ensure that restrictive access is in place.

 

12. In the main.tf file, change the instance type of server1 to t3.micro:

resource "aws_instance" "server1" {
    ami           = "ami-01fee56b22f308154"
    instance_type = "t3.micro"
    monitoring = true
    vpc_security_group_ids = [aws_vpc.example.default_security_group_id]
    subnet_id = aws_subnet.example.id               
    root_block_device {
        delete_on_termination = false
        encrypted = true
        volume_size = "8"
        volume_type = "standard"
    }
    tags = {
        Name = "calabvm1"
    }
}
 

13. Save the main.tf file by clicking File in the upper left corner and click save:

14.  Run terraform apply:

terraform apply

The execution plan shows the changes to be made. The EC2 instance will be updated to a t3.micro size because the configuration in main.tf has changed:

15. Input yes to deploy the config.

The EC2 instance is updated:

The state file is also updated with the new size:

The state file is a copy of all the resources that Terraform deploys and changes. What happens if the state file is deleted?

16. Delete the state file by inputting the following command into the terminal:

rm terraform.tfstate

17. In the terminal run Terraform plan:

terraform plan

Because the state file is gone, Terraform will try to rebuild the AWS resources because it thinks they are not there:

The state file is used as the source of truth then deploying Terraform resources. This is also why it's important to never directly modify the metadata inside the Terraform state.

18. Copy the terraform.tfstate.backup file to restore the state:

cp terraform.tfstate.backup terraform.tfstate

Everytime terraform plan or terraform apply is run, a backup is taken of the state file. You are using the backup file to recover the missing state file. If you were to run terraform plan again, you would see no infrastructure changes to be made because the state file is now accurate again.

19. In the main.tf file, remove the resource block for server2.

The main.tf file should look like the following with server2 removed:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
resource "aws_vpc" "example" {
  cidr_block = "10.0.0.0/16"
}
resource "aws_subnet" "example" {
  vpc_id     = aws_vpc.example.id
  cidr_block = "10.0.1.0/24"
  availability_zone = "us-west-2a"
  tags = {
    Name = "calabvm-subnet"
  }
}
resource "aws_instance" "server1" {
    ami           = "ami-01fee56b22f308154"
    instance_type = "t3.micro"
    monitoring = true
    vpc_security_group_ids = [aws_vpc.example.default_security_group_id]
    subnet_id = aws_subnet.example.id               
    root_block_device {
        delete_on_termination = false
        encrypted = true
        volume_size = "8"
        volume_type = "standard"
    }
    tags = {
        Name = "calabvm1"
    }
}
 

 20. Save the main.tf file by clicking File in the upper left corner and click save:

21. In the terminal run terraform plan:

terraform plan

The execution plan shows that Terraform will try to remove server2 because it is no longer in the configuration file but was previously in the state file:

To perform the destroy, run

terraform apply

The key takeaways to remember:

Terraform is declarative, infrastructure is described in the configuration file, and Terraform handles the automation and provisioning of those resources.  
The Terraform configuration file represents the current infrastructure. It will become unusable if the configuration becomes inaccurate due to manually modifying the AWS resources instead of using Terraform.
Store the state file in a restricted location because it can contain sensitive information like passwords and keys.
 

Summary
In this lab step, you explored Terraform state and how Terraform uses it to keep track of resources.

## 12:45

Introduction

Terraform state is stored locally by default in a terraform.tfstate file. With remote state, Terraform writes to a state file hosted in a remote data store. This provides a few advantages over a local state file like security, version control, and centralized storage. It also provides state locking, which is where only one person can modify the state file at a time, which prevents teammates from writing over each other. In AWS you can use an S3 bucket for storing the state file and a DynamoDB table for state locking.

In this lab step, you will create a DynamoDB table and S3 bucket and configure Terraform to use remote state.

2.  Left-click on the Terminal menu and click New Terminal:

A terminal window will appear at the bottom.

3. Inside the terminal, input the following commands to create an S3 bucket:

`S3NAME="terraformstate$(date | md5sum | head -c 8)" `

aws s3api create-bucket \
    --bucket $S3NAME \
    --region us-west-2 \
    --create-bucket-configuration \
    LocationConstraint=us-west-2
The remote state file will be stored in this S3 bucket.

 

4. Input the commands to enable encryption on the S3 bucket:

aws s3api put-bucket-encryption \
    --bucket $S3NAME \
    --server-side-encryption-configuration={\"Rules\":[{\"ApplyServerSideEncryptionByDefault\":{\"SSEAlgorithm\":\"AES256\"}}]}
The state file contains sensitive data to the infrastructure managed by it. It is important to enable server-side encryption on the S3 bucket. This lab uses AES256 however, it is recommended to use KMS and implement key rotations. 

5. Enable versioning on the bucket:

`aws s3api put-bucket-versioning --bucket $S3NAME --versioning-configuration Status=Enabled`

Versioning allows multiple versions of the state file to be saved which provides roll back safety for the state.

6. Create a DynamoDB table using the following command:

aws dynamodb create-table \
    --table-name terraform-state-lock \
    --attribute-definitions \
        AttributeName=LockID,AttributeType=S \
    --key-schema \
        AttributeName=LockID,KeyType=HASH \
    --region us-west-2 \
    --provisioned-throughput \
        ReadCapacityUnits=20,WriteCapacityUnits=20
You have created a DyanmoDB table that will store the current lock status of the remote state.

Storing the state remotely is great for centralized storage. However, there is risk of multiple teammates working from the same state file at the same time. If you understand how state works, you know that this would cause major issues when managing Terraform resource with state. State locking is a feature that prevents that state from being opened when it is already in use. The DynamoDB table is used by Terraform to set and unset the state locks. The AWS infrastructure is in place to use remote state in a Terraform configuration.

7. Right-click on the terraformlab folder and click New File:

8. Name the file main.tf and click OK:

In the lower right corner, you will see some pop-ups from the Terraform Visual Studio Code extension. These can be ignored:

9. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
  backend "s3" {
    bucket = "RENAMEME!"
    key    = "calabs/production/us-west-2/rslab/terraform.tfstate"
    region = "us-west-2"
    dynamodb_table = "terraform-state-lock"
    encrypt        = true
  }
}

provider "aws" {
  region = "us-west-2"
}

resource "aws_vpc" "example" {
  cidr_block = "10.0.0.0/16"
}

resource "aws_subnet" "example" {
  vpc_id     = aws_vpc.example.id
  cidr_block = "10.0.1.0/24"
  availability_zone = "us-west-2a"

  tags = {
    Name = "calabvm-subnet"
  }
}
The resource blocks contain the infrastructure to create with Terraform, in this lab it is just a simple VPC and subnet. 

The backend block refers to how Terraform operates with the state file. By default, the local backend is used, but creating a backend block within the Terraform configuration block tells Terraform to use an S3 bucket to store the state file:

It is important to use well thought out naming standards for the key argument. With advanced Terraform configurations, it is not uncommon to retrieve data from other states, and having solid naming in place will complement this process. The key argument includes the path and name of the state file. Also, notice the dynamodb_table argument is used with the name of the DynamoDB table to use for keeping track of state locking.

10. Save the main.tf file by clicking File in the upper left corner and click save:


11. Inside the terminal, input the following command to change directory to the terraformlab folder:

cd terraformlab

12.  Use the sed command to change out the S3 bucket name with the one we created:

`sed -i 's/RENAMEME!/'"${S3NAME}"'/g' main.tf`

Because the S3 bucket must be a unique name in AWS, this is an additional step in this lab in order to use the unique S3 bucket name that was created in earlier steps.

13. Initialize the directory for Terraform by inputting the terraform init command into the terminal:

terraform init

The AWS provider is downloaded during the initialization process. 

14. Run terraform apply:

terraform apply

During the terraform apply, the state file is locked. The DynamoDB table is holding the lock status of the file. If another user were to run terraform apply using the same remote state file right now, they would get a lock error. This is great to prevent multiple users from modifying the same infrastructure at once.

15.  Input yes to deploy the config.

The AWS resources are created:

16. Use the following command to view the state file within the S3 bucket:


`aws s3 ls s3://$S3NAME/calabs/production/us-west-2/rslab/`

When looking at the terraformlab directory, there is no local state file, however, there is a temporary one under the .terraform folder:

Summary
In this lab step, you created an S3 bucket and DynamoDB table and used them to configure remote state for a terraform configuration.

## 13:03

Introduction

In Terraform, you can create modules to build re-usable infrastructure components. For example, a module can exist for a MySQL environment and a separate one can exist for EC2 Instances. The modules can then be used in combination like building blocks to build and deploy AWS services for various environments. 

In this lab step, you will create a module for deploying EC2 Instances and use it in a Terraform configuration.

2. Right-click on the terraformlab folder and click New Folder:

3. Name the folder modules and click OK:

You will create a module for deploying an EC2 instance. 

4. Right-click on the modules folder and click New Folder:

5. Name the folder ec2 and click OK:

6. Right-click on the ec2 folder and click New File:

7. Name the file main.tf and click OK:

Note: In the lower right corner, you will see some pop-ups from the Terraform Visual Studio Code extension. These can be ignored:

8. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">=3.7.0"
    }
  }
}
resource "aws_instance" "server" {
    ami           = "ami-0d398eb3480cb04e7"
    instance_type = var.instance_size
    monitoring = false
    vpc_security_group_ids = var.security_group_ids
    subnet_id = var.subnet_id          
    root_block_device {
        delete_on_termination = false
        encrypted = true
        volume_size = 20
        volume_type = "standard"
    }
    tags = {
        Name = var.servername
    }
}
This contains the resource block for an EC2 Instance resource. It contains the EC2 standard configurations to be made into a module.  Also, notice that the required_providers block is specified. Because the AWS provider is updated so frequently, it's best practice to specify the version that your module is compatible with. Otherwise newer or older versions of the provider may not work with the module and cause issues.

 9. Right-click on the ec2 folder and click New File:

10. Name the file variables.tf and click OK:

11. Copy and paste the terraform variable blocks to the variables.tf file:

variable "instance_size" {
    description = "Size of the EC2 instance"
    type = string
    default = "t2.micro"
}
variable "servername" {
    description = "Name of the EC2 Instance tag"
    type = string
}
variable "subnet_id" {
    description = "Subnet ID assigned to the EC2 Instance"
    type = string
}
variable "security_group_ids" {
    description = "Security group IDs assigned to the EC2 Instance"
    type = list(string)
}
The variables will be used as input for the module to enable the EC2 to be customized per environment. The module will take the input for subnet and security group IDs to allow the module to be deployed to various environments. This makes the module reusable since the environment isn't hardcoded into the Terraform configuration. 

 12. Right-click on the ec2 folder and click New File.

13. Name the file outputs.tf and click OK.

14. Copy and paste the terraform output blocks to the outputs.tf file:

output "id" {
    description = "id of EC2"
    value = aws_instance.server.id
}
The output block will allow the module to output the ID value of the EC2 so it can be passed through to other resources within a Terraform configuration. 

Now, the module has been completed. You'll notice that this looks just like a regular Terraform configuration.  Terraform modules are just snippets of code that can be called from within other Terraform configurations. Think of it as how functions are used from a software development perspective. Functions are used to isolate actions or processes in code, which allows them to be tested, decoupled, and reused throughout the application. In Terraform, the concept is still the same, but they are called modules. 

15. Right-click on the terraformlab folder and click New File:

16. Name the file main.tf and click OK:

17. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
resource "aws_vpc" "prod" {
  cidr_block = "10.0.0.0/16"
}
resource "aws_subnet" "sub1" {
  vpc_id     = aws_vpc.prod.id
  cidr_block = "10.0.1.0/24"
  availability_zone = "us-west-2a"
  tags = {
    Name = "subnet"
  }
}
module "webserver" {
    source = "./modules/ec2"
    servername = "calabvm"
    instance_size = "t2.micro"
    subnet_id = aws_subnet.sub1.id
    security_group_ids = [aws_vpc.prod.default_security_group_id]
}
resource "aws_ec2_tag" "tags" {
  resource_id = module.webserver.id
  key         = "Environment"
  value       = "Production"
}
To use the module within the Terraform configuration, the resource type is declared as a module type. The block label can be any name given to the module block. In this example, it's webserver. The arguments that parametrize the module are included in the module block. The source argument defines the location of the module that is being used. In this case, it's within the child folders of the directory. Next, the arguments that were declared in the variables.tf file are specified. Notice that the subnet_id and security_group_ids arguments are passing information from the aws_vpc and aws_subnet resource blocks. This is how you can pass data into modules:

Next, notice in the aws_ec2_tag resource block, the module output id is used by specifying module with the module name. In this example, the value of id is declared from the outputs.tf file of the ec2 module. This is how modules can create data and pass it along to other resources:

18. Save the terraform configuration files by clicking File in the upper left corner and click Save All:

19. Left-click on the Terminal menu and click New Terminal:

A Terminal window will appear at the bottom.

20. Inside the terminal, input the following command to change directory to the terraformlab folder:

cd terraformlab
 

21. Initialize the directory for Terraform by inputting the terraform init command into the terminal:

terraform init

The AWS provider is downloaded during the initialization process. Also notice the module is initialized. Since the module is sourced locally within a child folder, it is not necessary to download the module. However, modules can be sourced in other places like the Terraform registry and would be downloaded during the initialization phase.

22. Run terraform apply:

terraform apply
 

23. Input yes to deploy the config:

The EC2 module has been used to deploy the EC2 instance. In this example, you created a module and stored it in a child folder. You can also use other sources for modules like version control repositories like GitHub. HashiCorp also has a public module registry called the Terraform Registry, which hosts community-made modules. 

Summary
In this lab step, you created a module for deploying EC2 instances and called the module within a Terraform configuration file.

## 13:16

Introduction

Importing existing infrastructure into Terraform is a slow process that must be done with caution. One of the key takeaways is to understand that importing infrastructure does not create a Terraform configuration automatically. The configuration must be created manually. The typical workflow for importing existing infrastructure into Terraform is as follows:

Existing infrastructure is imported using the terraform import command, then the Terraform configuration file is modified to sync with the existing infrastructure settings. Afterward, terraform plan and terraform apply are used to verify both the Terraform configuration, state, and existing resources are in sync. 

In this lab step, you will import an existing VPC resource in AWS into a Terraform configuration and state file.

2. Right-click on the terraformlab folder and click New File:

3. Name the file main.tf and click OK:

It's common practice to name Terraform configuration files "main". You can also create multiple .tf files in a directory which will be explored in future labs.

4. Copy and paste the Terraform resource blocks to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
resource "aws_vpc" "dev" {}
An empty aws_vpc resource block is created as a place holder for the existing VPC in AWS. Once the resource is imported into a state file, this resource will need to be modified with the proper arguments:

5. Save the main.tf file by clicking File in the upper left corner and click save:

6. Left-click on the Terminal menu and click New Terminal:

A Terminal window will appear at the bottom.

7. Inside the terminal, input the following command to change directory to the terraformlab folder:

cd terraformlab

8. Initialize the directory for Terraform by inputting the terraform init command into the terminal:

terraform init

The AWS provider is downloaded during the initialization process. 

9. Input the following command to get the VPC ID of the VPC to import into Terraform:

Terraform requires the ID of the resource to be imported. In this case, the VPC ID is required since a VPC is being imported. If a Subnet were to be imported, Terraform would require a Subnet ID:

10. Use terraform import specifying the aws_vpc.dev resource block and VPC ID:

`terraform import aws_vpc.dev $VpcID`

The empty aws_vpc.dev resource block in the main.tf file is mapped to the VPC ID in AWS. A state file is then created with the attributes of the VPC:

This is why the empty aws_vpc.dev resource block needs to be created first. Terraform needs to map the resource ID from the configuration file with the AWS ID.

11. In the terminal use the terraform show command:

terraform show

The resource block for the VPC is displayed. The terraform show command, displays human-readable output from the state file.

12. Copy the resource block output from the terraform show command:

13. In the main.tf file, overwrite the empty aws_vpc resource block by pasting over it with the new resource block content that was previously copied:

14.  Save the changes to the main.tf file.

15.  Run terraform plan to validate the configuration:

terraform plan

The aws_vpc resource block that was copied to the configuration contains arguments that cannot be managed with Terraform. These are mostly identifier attributes. 

16. In the main.tf file, remove each argument that cannot be set. The entire main.tf file should look like the following when done:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
resource "aws_vpc" "dev" {
    assign_generated_ipv6_cidr_block = false
    cidr_block                       = "192.168.100.0/24"
    enable_classiclink               = false
    enable_classiclink_dns_support   = false
    enable_dns_hostnames             = true
    enable_dns_support               = true
    instance_tenancy                 = "default"
    tags                             = {
        "Name" = "Web VPC"
    }
}

17. Save the changes made to the main.tf file.
 
18. Use the terraform plan command again to validate the configuration:

terraform plan

The execution plan shows that the AWS resource and Terraform configuration are in sync. 

Notice: there are some cases where a terraform apply will need to be done in order to update existing values:

It is important to understand how each AWS resource is imported into Terraform and highly advised to test before importing production infrastructure. You should not rely on the provider documentation to determine the outcome of the import. 

Importing existing infrastructure into Terraform can be a tedious process. However, the benefits of managing pre-existing infrastructure with Terraform are great and worth the effort. HashiCorp is currently in the process of improving the import experience and will be making efforts in the future to speed up the import process. You may also want to check out the community-made tool, Terraformer, that automates the Terraform import process for the major cloud providers. 

Summary
In this lab step, you successfully imported an AWS VPC into a Terraform configuration that can now be managed with Terraform.

## 15:22

Introduction

Creating conditional logic within Terraform configurations allows for creating a more dynamic configuration. This allows for greater abstraction and logic flow in the code. It's essential to understand the logical conditions that can be created within the HCL (HashiCorp Configuration Language) language as it provides even more reusability.

In this lab step, you will create a module with conditional logic to make it more dynamic.

2. Right-click on the terraformlab folder and click New Folder:

3. Name the folder modules and click OK:

You will create a module for deploying an EC2 instance. It is recommended to develop configurations with modules in mind. Local modules can be used to organize code. In this lab, the following folder hierarchy will be used:

terraformlab
    └──modules 
            └──ec2
                └── main.tf
                └── variables.tf
 

4. Right-click on the modules folder and click New Folder:

5. Name the folder ec2 and click OK:

6. Right-click on the ec2 folder and click New File:

7. Name the file main.tf and click OK:


Note: In the lower right corner, you will see some pop-ups from the Terraform Visual Studio Code extension. These can be ignored:

8. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">=3.7.0"
    }
  }
}
data "aws_ami" "default" {
  most_recent = "true"
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-*"]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
  owners = ["099720109477"]
}
resource "aws_instance" "server" {
    ami           = var.ami != "" ? var.ami : data.aws_ami.default.image_id
    instance_type = var.instance_size
    tags = {
        Name = "calabvm"
    }
}
The data block is a data resource type that is used to collect data from the provider. In this case, the data block is of the aws_ami resource type, which means the block is used to collect the available AMI in the AWS environment. The filter and owner arguments are used to specify the AMI type to retrieve:


The resource block aws_instance contains the logic for the AMI to use. The logic within the expression is called a conditional expression. It follows the following format:


condition ? true : false

The condition is if the variable AMI does not contain an empty string. If it is true, then set the AMI to the value of var.ami. If it is false, set the value of the AMI to the ID of data.aws_ami.default[0].image_id which is the AMI ID that was collected in the data block. This strategy gives the module the ability to take in AMI ID input or find an AMI on its own and makes the module more dynamic:

The use of != is the inequality operator. They can be used within HCL to define true or false reasoning. There are other types of operators as well. 

 9. Right-click on the ec2 folder and click New File:

10. Name the file variables.tf and click OK:

11. Copy and paste the terraform variable blocks to the variables.tf file:

variable "servername"{
    description = "Name of the server"
    type = string
}
variable "ami" { 
    description = "AMI ID to deploy"
    type = string
    default = ""
}
variable "instance_size" {
    description = "Size of the EC2 instance"
    type = string
    default = "t2.micro"
}
The important thing to note about the variables blocks is the AMI variable contains a default value of an empty string:

The empty string is the trigger for the conditional logic in main.tf. It allows the user of the EC2 module to omit an AMI ID value and have the module collect an AMI ID value on its own. 

12. Right-click on the terraformlab folder and click New File:

13. Name the file main.tf and click OK:

14. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
module "webserver" {
    source = "./modules/ec2"
    servername = "calabvm"
    instance_size = "t2.micro"
}

15. Save the terraform configuration files by clicking File in the upper left corner and click Save All:
 
16. Left-click on the Terminal menu and click New Terminal:

A Terminal window will appear at the bottom.

17. Inside the terminal, input the following command to change directory to the terraformlab folder:

cd terraformlab

18. Initialize the directory for Terraform by inputting the terraform init command into the terminal:

terraform init

The AWS provider is downloaded during the initialization process. Also, notice the module is initialized. 

19. Run terraform plan:

terraform plan

Notice the AMI data block ran to find an AMI ID in AWS that meets the filter requirements. The execution plan shows the resulting AMI ID value:

20. Add the AMI argument to the module block:

module "webserver" {
    source = "./modules/ec2"
    ami = "ami-0528a5175983e7f28"
    servername = "calabvm"
    instance_size = "t2.micro"
}
The entire contents of main.tf should look like the following:

Copy code
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}
provider "aws" {
  region = "us-west-2"
}
module "webserver" {
    source = "./modules/ec2"
    ami = "ami-0528a5175983e7f28"
    servername = "calabvm"
    instance_size = "t2.micro"
}
This will test out the module's ability to optionally take AMI ID input.

21. Save the changes to the main.tf file.

22. Run terraform apply:

terraform apply

Notice that the AMI ID value has been set to the one specified in the module:

23. Input yes to deploy the config:

The EC2 module has been deployed successfully by specifying the AMI ID. The conditional logic created in the Terraform module makes it dynamic and allows it to  be used in several different scenarios. 

Summary
In this lab step, you created an EC2 Instance module that uses a conditional expression to optionally take AMI ID input to deploy an EC2 Instance.

## 15:36

Introduction
Using loops in Terraform can give code a cleaner look and follows the DRY (Don't Repeat Yourself)  principles of programming where the same concepts aren't repeated throughout the Terraform configuration. Loops also allow for resources to scale efficiently. 

You will create a module for deploying an EC2 instance. It is recommended to develop configurations with modules in mind. Local modules can be used to organize code. In this lab, the following folder hierarchy will be used:

Copy code
terraformlab
    └──modules 
            └──ec2
                └── main.tf
                └── variables.tf
In this lab step, you will create a module with loops using several methods.

2. Right-click on the ec2 folder and click New File:

3. Name the file variables.tf and click OK:

4. Copy and paste the terraform variable blocks to the variables.tf file:

variable "associate_public_ip_address" {
  description = "Associate a public IP address with EC2 instance"
  type        = bool
  default     = true
}
variable "servername"{
    description = "Name of the server"
    type = string
}

#Variable that will contain a list of map types
variable "ebs_block_device" {
  description = "Additional EBS block devices to attach to the instance"
  type        = list(map(string))
  default     = []
}
Notice the ebs_block_device variable is the type of list(map(string)). This indicates that this variable can hold a list of several maps:

In the next steps, you will create a configuration with a loop that iterates through all specified maps in the  ebs_block_device variable and creates an EBS volume with their values. You can also create a list of objects as well instead of maps, which would be appropriate if each value was of a different type like bool, string, and integer.

5. Right-click on the ec2 folder and click New File:

6. Name the file main.tf and click OK:

Note: In the lower right corner, you will see some pop-ups from the Terraform Visual Studio Code extension. These can be ignored:

7. Copy and paste the terraform resource block to the main.tf file:

resource "aws_instance" "server" {
    ami           = "ami-0528a5175983e7f28"
    instance_type = "t2.micro"
    associate_public_ip_address = var.associate_public_ip_address

    #dynamic block with for_each loop
    dynamic "ebs_block_device" {
    for_each = var.ebs_block_device
        content {
        delete_on_termination = lookup(ebs_block_device.value, "delete_on_termination", null)
        device_name           = ebs_block_device.value.device_name
        encrypted             = lookup(ebs_block_device.value, "encrypted", null)
        iops                  = lookup(ebs_block_device.value, "iops", null)
        kms_key_id            = lookup(ebs_block_device.value, "kms_key_id", null)
        snapshot_id           = lookup(ebs_block_device.value, "snapshot_id", null)
        volume_size           = lookup(ebs_block_device.value, "volume_size", null)
        volume_type           = lookup(ebs_block_device.value, "volume_type", null)
        }
    }

    tags = {
        Name = "${var.servername}"
    }
}
In the aws_instance resource block there is a dynamic block referenced for ebs_block_device:

Dynamic blocks can be used for resources that contain repeatable configuration blocks. Instead of repeating several ebs_block_device blocks, a dynamic block is used to simplify the code. This is done by combining the dynamic block with a for_each loop inside. The first line inside the dynamic block is the for_each loop. The loop is iterating through the list of the ebs_block_device variable, which is a list of maps. In the content block, each value of the map is referenced using the lookup function. The logic here is to look for a value in the map variable and if it's not there, set the value to null. The dynamic block will iterate through each map in the list: 

8. Add the aws_eip resource block to the main.tf file:

#public IP address with Count Conditional Expression
resource "aws_eip" "pip" {
  count             = var.associate_public_ip_address ? 1 : 0
  network_interface = aws_instance.server.primary_network_interface_id
  vpc               = true
}

In the aws_eip resource block is the count parameter:

Count allows for creating multiple instances of a resource block. Almost all resource blocks can use the count attribute. It is simply the number of times to create the resource block. It can also be used as conditional logic. In this case, the value of count is a conditional expression. If var.associate_public_ip_address is true set the count value to 1, if false set it to 0. This allows resource blocks to be created conditionally. In this example, a public IP address is not created if var.associate_public_ip_address is set to false.

After adding the aws_eip resource block, the entire main.tf file should look like the following:

resource "aws_instance" "server" {
    ami           = "ami-0528a5175983e7f28"
    instance_type = "t2.micro"
    associate_public_ip_address = var.associate_public_ip_address

    #dynamic block with for_each loop
    dynamic "ebs_block_device" {
    for_each = var.ebs_block_device
        content {
        delete_on_termination = lookup(ebs_block_device.value, "delete_on_termination", null)
        device_name           = ebs_block_device.value.device_name
        encrypted             = lookup(ebs_block_device.value, "encrypted", null)
        iops                  = lookup(ebs_block_device.value, "iops", null)
        kms_key_id            = lookup(ebs_block_device.value, "kms_key_id", null)
        snapshot_id           = lookup(ebs_block_device.value, "snapshot_id", null)
        volume_size           = lookup(ebs_block_device.value, "volume_size", null)
        volume_type           = lookup(ebs_block_device.value, "volume_type", null)
        }
    }

    tags = {
        Name = "${var.servername}"
    }
}

#public IP address with Count Conditional Expression
resource "aws_eip" "pip" {
  count             = var.associate_public_ip_address ? 1 : 0
  network_interface = aws_instance.server.primary_network_interface_id
  vpc               = true
}
 

9. Right-click on the terraformlab folder and click New File:

10. Name the file main.tf and click OK:

11. Copy and paste the terraform resource block to the main.tf file:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}

provider "aws" {
  region = "us-west-2"
}


module "server" {
    source = "./modules/ec2"

    servername = "testserver"
    associate_public_ip_address = true

    #The input for the list of maps variable created in the EC2 module
    ebs_block_device = [
        {
        device_name = "/dev/sdh"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        }
    ]

}

12. Save the terraform configuration files by clicking File in the upper left corner and click Save All:

13. Left-click on the Terminal menu and click New Terminal:

A Terminal window will appear at the bottom.

14. Inside the terminal, input the following command to change directory to the terraformlab folder:

cd terraformlab

15. Initialize the directory for Terraform by inputting the terraform init command into the terminal:

terraform init

The AWS provider is downloaded during the initialization process. Also, notice the module is initialized. 

16. Run terraform plan:

terraform plan

In the execution plan, notice the public IP address resource will be created due to the count conditional logic:

Since only one map was specified, only one ebs_block_device is created:

17. Add another ebs block to the module and set associate_public_ip_address to false :

module "server" {
    source = "./modules/ec2"

    servername = "testserver"
    associate_public_ip_address = false

    ebs_block_device = [
        {
        device_name = "/dev/sdh"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        },
        {
        device_name = "/dev/sdj"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        }
    ]

}
The entire contents of main.tf should look like the following:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}

provider "aws" {
  region = "us-west-2"
}

module "server" {
    source = "./modules/ec2"

    servername = "testserver"
    associate_public_ip_address = false

    ebs_block_device = [
        {
        device_name = "/dev/sdh"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        },
        {
        device_name = "/dev/sdj"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        }
    ]

}
 

18. Save the changes to the main.tf file.

19. Run terraform apply:

terraform apply

In the execution plan, the public IP address resource will no longer be created since associate_public_ip_address was set to false. Also, there are now two ebs volumes to be created since an additional map was added to the module configuration:

20. Input yes to deploy the config:

The EC2 module has been deployed successfully using the for_each loop to create the ebs volumes.

21. Add a count argument to the module block :

module "server" {
    count = 3 
    source = "./modules/ec2"

    servername = "testserver${count.index}"
    associate_public_ip_address = false

    ebs_block_device = [
        {
        device_name = "/dev/sdh"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        },
        {
        device_name = "/dev/sdj"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        }
    ]

}
The servername argument contains the count.index variable in the value. Count.index becomes available when using count. It represents the index value of the resource being created. It can be used to provide a unique name to resources when they are duplicated with count. In this example, the server name of the 3 servers will be testserver0, testserver1, and testserver2.

The entire contents of main.tf should look like the following:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "3.7"
    }
  }
}

provider "aws" {
  region = "us-west-2"
}

module "server" {
    count = 3 
    source = "./modules/ec2"

    servername = "testserver${count.index}"
    associate_public_ip_address = false

    ebs_block_device = [
        {
        device_name = "/dev/sdh"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        },
        {
        device_name = "/dev/sdj"
        volume_size = "4"
        volume_type = "standard"
        delete_on_termination = "true"
        }
    ]

}
In Terraform v0.13 the count loop can now be used on modules. Now you can create modules and use loops to scale them!

22. Save the changes to the main.tf file.

23. Run terraform plan:

terraform plan

In the execution plan, three new servers will be deployed and the existing server will be destroyed:

This is because the count argument turns the module into an indexed resource which forces the existing one to be destroyed:

Summary
In this lab step, you created an EC2 Instance module that contains a for_each loop for deploying EBS volumes. You also used count to create conditional logic for the public IP address resource and scale the module to deploy 3 servers. 

