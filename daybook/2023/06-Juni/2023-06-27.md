# Notes for 2023-06-27

## 12:56

### Gitlab Pipelines

*Setup and pre-stage .cache*:

  Each stage is represented as a container which cannot interact
  with another container without cached files, etc.

  In the Setup stage all npm installs are being applied and the
  build folder (folder to be tested) is being build.

*Test*:

  In the Test stages, each part of the monorepo is being tested.
  They get the build folder from the Cache (to speed up the pipeline)
  and then run the tests for each module eg. backend, shared-model,
  etc.

  The test should only run on a merge-request-event on the default
  branch which in this case would be `main`.

  For the data-layer there are bugs in the test phase that have to be
  fixed, by Lennart or Stefan.

*Test-push*:

  Is a debugging step. 
  Here all used and declared variables for the pipeline are being
  exported with the values. 
  To show how or why a certain step failed.

*Deploy*:

  In the deploy stage the docker images are being build and tagged.
  You can only push an image that has an tag with an correct
  semantic version meaning v+major.minor.path+pre-release(eg. alpha,
  beta, etc).

  You need an different image in this case to have the aws-cli for
  this stage.
  You apply the git-tags to the docker
  image, log in to aws ecr and then push it to ecr.

  The last step in this stage would be to push the build docker
  image to the Elastic Container Registry. This should be triggered
  manually, to prevent pushing 2 docker images with the same tag to
  the registry.


*Used keywords*:

  *image:* - used to declare the image used for the pipeline. In
  case of setup and test. This should be node because we used it for
  the project. Could be verified with package.json or if you have
  installed nvm, you should get a message with the version while
  cd-ing into the project folder.

  *stages:* - used as a descriptor for the different phases of the
  pipeline, eg. test-stage will run tests etc. You could name these
  as you like. As an example to run the setup stage and give it a
  more precise description you could do the following:

    ```yaml

    # Job description, that is shown in the overview on gitlab web
    setup:install-dependencies:
      # Stage description given in the stage section on top of the yml
      # file, this has to match the description under the stage keyword 
      stage: setup
      ....

    ``` 
    
  From the official Gitlab pipelines page: 
  - Jobs, define what to do. For example, jobs that compile or test
    code
  - Stages define when to run a job(s). For example, stages that run
    tests after stages that compile code.

  *cache:* - the cache keyword is used for dependencies, like
  packages you download or install via npm. Cache is stored where
  GitLab Runner is installed and uploaded to S3 if distributed cache
  is enabled.

  - Define cache per job by using the cache keyword. Otherwise it  is disabled.
  - Subsequent pipelines can use the cache.
  - Subsequent jobs in the same pipeline can use the cache, if the
    dependencies are identical.
  - Different projects cannot share the cache.
  - By default, protected and non-protected branches do not share the
    cache. However, you can change this behavior.

    Questions:
    - Is it maybe better to use a distributed cache, for security
      reasons?

  *rules:* - Here we use mostly if rules to run a stage on a given
  event. Eg. run the pipeline only if it is a merge-request and
  commit to main branch.
  Or Only run pipeline if the commit has a tag and is being pushed.

  *script:* - run the commands or login commands to do what was
  asked from the task. Eg. run the tests, login to ecr. Push the
  image to ECR, etc.
  Could also be custom bash script that is in the repository. Or it
  could be any bash command, eg. `echo $VAR` for debugging.
   


## 14:31

A manual job stage doesn't start automatically, and the pipeline can
complete without it starting. The Manual stage has allow_failure: true
by default.
The stage which is being manual has a play button in the gitlab web overview.

Questions:
  - Should we use manual for the Deploy stage? 
    So that the Pipeline succeeds when tested.
  - How is this being notified?
    - Is there a messaage being send to the dev which is in
      "charge"? 
    - How can you configure this properly?

If you want the non-manual stage depend on the manual one for
completion, use the needs keyword.

The `needs` keyword creates a dependency between two jobs
regardless of their stage, and the stage ordering. The `needs` keyword lets you create a
`Directed Acyclic Graph (DAG)` to speed up the pipeline.

You could overwrite the succeeding on manual if you add the keyword
`allow_failure` and set it to `false`.

## 14:59

### Terraform

*Benefits of Infrastructure as Code*:

- You can automate your entire provisioning and deployment process,
  which makes it much faster and more reliable than any manual
  process.
- You can represent the state of your infrastructure in source files
  that anyone can read rather than in a sysadmin's head.
- You can store those source files in version control, which means the
  entire history of your infrastructure is now captured in the
  commit log, which you can use to debug problems, and if necessary,
  roll back to older versions.
- You can validate each infrastructure change through code reviews and
  automated tests.
- You can create (or buy) a library of reusable, documented,
  battle-tested infrastructure code that makes it easier to scale and
  evolve your infrastructure.

## 15:26

*Configuration Management Needs*:

In particular, if you use Docker or Packer the vast majority of your
configuration management needs are already taken care of. With
Docker or Packer, you can create images (such as containers or
virtual machine images) that have all the software your server needs
already installed and configured. Once you have such an image, all you
need is a server to run it. And if all you need to do is provision a
bunch of servers, then a provisioning tool like Terraform is
typically going to be a better fit than a configuration management tool
(like Ansible). Here is an example of how to use Terraform to deploy
Docker to AWS:
 - https://github.com/brikis98/infrastructure-as-code-talk

If you're not using server templating tools, a good alternative is to
use a configuration management and provisioning tool together. For
example, a popular combination is to use Terraform to provision your
servers and Ansible to configure each one.

## 15:34

*Mutable vs Immutable*:

Configuration management tools such as Chef, Puppet and Ansible
typically default to a `mutable` infrastructure paradigm. Over time, as
you apply changes of software versions (eg update openssl several
times), each server builds up a unique history of changes. As a
result, each server becomes slightly different than all the others,
leading to subtle configuration bugs that are difficult to diagnose
and reproduce (configuration drift). 
If you're using a provisioning tool such as Terraform to deploy
machine images created by Docker or Packer, most "changes" are
actually deployments of a completely new server. Because every
deployment uses `immutable` images on fresh servers, this approach
reduces the likelihood of configuration drift bugs, makes it easier
to know exactly what software is running on each server, and allows
you to easily deploy any previous version of the software (any
previous image) at any time.
It also makes your automated testing more effective, because an
immutable image that passes your tests in the test environment is
likely to behave exactly the same way in the production environment.
